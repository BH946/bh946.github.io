---
title: "01. Backpropagation(DL 역전파)"
categories: Machine_Learning
tag: [python, machine_learning, concept]
toc: true
toc_sticky: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../../..
---



**딥러닝의 핵심 개념인 역전파(Backpropagation) 알고리즘에 대해 설명합니다. 퍼셉트론과 뉴런 구조, 레이어 개념, 경사하강법(Gradient Descent Method), 손실 함수(Loss Function), 그리고 체인 룰(Chain Rule)과 활성화 함수를 통한 역전파 계산 과정을 다양한 예제와 함께 단계별로 설명하고 있습니다.**

<br><br>

# Backpropagation

**MLP 포스팅 게시글과 유사한 내용이긴 하다.**

<br><br>

## 1. Perceptron + 뉴런(노드) Process 

**뇌세포 == 뉴런(노드)** 

**wx+b => Perceptron**

![image-20230419215708787](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419215708787.png) 

* SLP : 히든레이어 X

<br>

### Perceptron 의 행렬곱

 ![1](/images/2023-04-14-01. Backpropagation(DL 역전파)/1.png) 

<br><br>

## 2. Layer(레이어)

**딥러닝(DL=DNN)이란 히든 레이어를 많이(깊게) 쌓은 MLP, CNN 등등 이런 구조를 칭한다.**

![image-20230419220004853](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220004853.png) 

<br><br>

## 3. Gradient Decent Method(GD method)

**아주 유명한 "경사하강법" 이며, 특정값을 찾아가는 방법이다.**

**초기값 W를 학습시켜서  "손실 함수"가 최소값(특정값)이 되는 W파라미터를 구해준다.**

![image-20230419220748737](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220748737.png) 

<br><br>

## 4. Loss(Cost) Function

**손실(비용,오차) 함수이므로 당연히 "최소화"가 목적이다.**

**"최소화" 를 위해서 경사하강법을 적용할 수 있다.**

![image-20230419220916875](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419220916875.png) 

<br><br>

## 5. Chain Rule과 Activation Function

**여기서 Activation Function는 `-4x` 로 볼 수 있다.**

![image-20230419221445676](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221445676.png) 

<br>

### 예제 1

![image-20230419221913700](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221913700.png) 

<br>

### 예제 2

![image-20230419221933802](/images/2023-04-14-01. Backpropagation(DL 역전파)/image-20230419221933802.png) 

<br>

### 예제 3

![1](/images/2023-04-14-01. Backpropagation(DL 역전파)/1-16819105797211.png) 
