---
title: "02. MLP(다층 퍼셉트론)"
categories: Machine_Learning
tag: [python, machine_learning, concept]
toc: true
toc_sticky: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../../..
---





# MLP(다층 퍼셉트론)

**이전 포스팅한 Backpropagation 게시물과 유사하며 참고해서 보는것을 추천한다.**

**목표는 arg min(오차) 이다.**

![image-20230419222502507](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222502507.png) 

<br><br>

## 1. 1,2-Layer Perceptron

**`2-Layter Perceptron` 은 중첩함수 같은 형태를 띄며 여기서 부턴 "히든 레이어" 가 존재하는걸로 볼 수 있다.**

**그렇다는 것은 MLP를 뜻하며, `1-Layter Perceptron` 의 경우 SLP로 볼 수 있따.**

![image-20230419222558003](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222558003.png) 

<br>

![image-20230419222834614](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222834614.png) 

* 참고로 FC는 Fully Connect로써 레이어에 노드들이 모두 연결되어있는 구조를 의미한다.

<br><br>

## 2. Activation Function

**활성화 함수도 당연히 다양하게 존재하며, 여러개를 소개한다.**

![image-20230419222947939](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222947939.png) 

<br>

![image-20230419223015610](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223015610.png) 

<br><br>

## 3. Loss Function

**손실, 오차, 비용 함수라고 하며 관련 함수들인 MAE, MSE, CEE 를 소개하겠다.**

![image-20230419223128861](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223128861.png) 

<br>

### 예제 - MSE

![image-20230419223233424](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223233424.png) 

<br>

### 예제 - CEE

![image-20230419223218071](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223218071.png) 

<br><br>

## 4. Optimization(EX:GD-method)

**Optimization 이란 손실 함수를 최소로 만드는 가중치를 찾는 알고리즘이다.**

![image-20230419223342891](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223342891.png) 

<br><br>

## 5. Batch Size(Epoch, Iteration)

**용어 소개**

![image-20230419223419094](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223419094.png) 
