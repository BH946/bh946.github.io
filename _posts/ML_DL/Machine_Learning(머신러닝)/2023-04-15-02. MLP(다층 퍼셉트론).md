---
title: "02. MLP(다층 퍼셉트론)"
categories: Machine_Learning
tag: [python, machine_learning, concept]
toc: true
toc_sticky: true
author_profile: false
sidebar:
  nav: "docs"
typora-root-url: ../../..
---



**다층 퍼셉트론(MLP)의 구조와 작동 원리를 설명합니다. 1층과 2층 퍼셉트론의 차이, 다양한 활성화 함수(시그모이드, 탄젠트, ReLU), 손실 함수(MAE, MSE, CEE)의 특성과 계산 예제, 경사하강법을 포함한 최적화 방법, 그리고 배치 크기와 에포크, 반복 횟수 개념을 체계적으로 다루고 있습니다.**

<br><br>

# MLP(다층 퍼셉트론)

**이전 포스팅한 Backpropagation 게시물과 유사하며 참고해서 보는것을 추천한다.**

**목표는 arg min(오차) 이다.**

![image-20230419222502507](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222502507.png) 

<br><br>

## 1. 1,2-Layer Perceptron

**`2-Layter Perceptron` 은 중첩함수 같은 형태를 띄며 여기서 부턴 "히든 레이어" 가 존재하는걸로 볼 수 있다.**

**그렇다는 것은 MLP를 뜻하며, `1-Layter Perceptron` 의 경우 SLP로 볼 수 있따.**

![image-20230419222558003](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222558003.png) 

<br>

![image-20230419222834614](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222834614.png) 

* 참고로 FC는 Fully Connect로써 레이어에 노드들이 모두 연결되어있는 구조를 의미한다.

<br><br>

## 2. Activation Function

**활성화 함수도 당연히 다양하게 존재하며, 여러개를 소개한다.**

![image-20230419222947939](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419222947939.png) 

<br>

![image-20230419223015610](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223015610.png) 

<br><br>

## 3. Loss Function

**손실, 오차, 비용 함수라고 하며 관련 함수들인 MAE, MSE, CEE 를 소개하겠다.**

![image-20230419223128861](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223128861.png) 

<br>

### 예제 - MSE

![image-20230419223233424](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223233424.png) 

<br>

### 예제 - CEE

![image-20230419223218071](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223218071.png) 

<br><br>

## 4. Optimization(EX:GD-method)

**Optimization 이란 손실 함수를 최소로 만드는 가중치를 찾는 알고리즘이다.**

![image-20230419223342891](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223342891.png) 

<br><br>

## 5. Batch Size(Epoch, Iteration)

**용어 소개**

![image-20230419223419094](/images/2023-04-15-02. MLP(다층 퍼셉트론)/image-20230419223419094.png) 
